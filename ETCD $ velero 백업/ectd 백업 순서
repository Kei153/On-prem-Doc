-- etcd 백업을 위한 스크립트 -- (버전별로 다를 수 잇다. 경로들이 어디있는지 확인 해 볼것)

** ❗️❗️etcd 정보를 얻을 때는 kubeadm이든, kubespray든, 버전이 몇이든 제일 먼저 해야 하는 건 현재 클러스터에서 etcd가 어떻게 실행되고 있는지 확인하는 것❗️❗️ **

💡 1단계. etcd가 어디서 어떻게 실행되는지 확인 : systemd 서비스인지, static pod인지를 먼저 확인
  # systemd 서비스 여부 확인
    systemctl status etcd
  # static pod 여부 확인 (kubeadm에서 자주 씀)
    ls -l /etc/kubernetes/manifests/etcd.yaml

💡2단계. 환경변수 또는 실행 옵션에서 인증서·엔드포인트 확인 - 여기서 ETCD_CERT_FILE, ETCD_KEY_FILE, ETCD_TRUSTED_CA_FILE, ETCD_LISTEN_CLIENT_URLS를 확인.
  #systemd 환경
    systemctl cat etcd
    cat /etc/etcd.env
  #static pod 환경
    grep -E -- '--cert-file|--key-file|--trusted-ca-file|--listen-client-urls' /etc/kubernetes/manifests/etcd.yaml

💡3단계. 엔드포인트 테스트
  ETCDCTL_API=3 etcdctl \
  --endpoints=https://< 주ETCD 있는 IP>:2379 \
  --cacert=<ca 경로> \
  --cert=<cert 경로> \
  --key=<key 경로> \
  endpoint status -w table



* kubespray *
(최신 kubespray 경로 확인한 명령어 : sudo cat /etc/etcd.env | grep -E 'ETCD_(NAME|DATA_DIR|ADVERTISE_CLIENT_URLS|LISTEN_CLIENT_URLS|TRUSTED_CA_FILE|CERT_FILE|KEY_FILE)=')


ETCDCTL_API=3 etcdctl snapshot save /home/<사용자>/<설정한 디렉토리명>/db.snapshot \
  --endpoints=https://(etcd 클러스터의 ip - 복제할 etcd가 있는 마스터 노드 ip):2379 \
  --cacert=/etc/kubernetes/ssl/etcd/ca.crt \
  --cert=/etc/kubernetes/ssl/apiserver-etcd-client.crt \
  --key=/etc/kubernetes/ssl/apiserver-etcd-client.key

Kubespray는 etcd도 Ansible로 설치. 인증은 클러스터 내 apiserver가 etcd에 접속할 때 사용하는 client 인증서 사용. /etc/kubernetes/ssl/apiserver-etcd-client.crt는 apiserver → etcd 인증에 사용
즉, etcd의 클라이언트로서 인증하므로 apiserver-etcd-client 인증서를 사용

* kubeadm *

ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /home/<사용자명>/<내가 지정한 디렉토리명>/db.snapshot

백업 시 etcd 서버로 인증 → server.crt/server.key 사용. etcd 서버가 나 자신이므로 server 인증서를 사용



-- etcd 복구 명령 --

* kubeadm *

etcdutl --data-dir <복구할 디렉토리 보통은 etcd 설치되어 있는 경로 : /var/lib/etcd-new> snapshot restore /home/<사용자명>/<내가 지정한 디렉토리명>/db.snapshot
1. 기존의 etcd 지우기 : mv /var/lib/etcd /var/lib/etcd-old
3. 새로운 etcd 옮기기 : mv /var/lib/etcd-new /var/lib/etcd

2. 기존의 etcd.yaml 지우기 : mv /etc/kubernetes/manifests/etcd.yaml /etc/kubernetes/manifests/etcd.yaml.bak
4. 새로운 etcd.yaml 옮기기 (etcd 재기동) : mv /etc/kubernetes/manifests/etcd.yaml.bak /etc/kubernetes/manifests/etcd.yaml

watch crictl ps 명령으로 static pod 들 재생성 되는지 확인.
kubectl get no 로 정상적으로 통신 되는지 확인


* kubespray *

1단계: 모든 etcd 노드에서 etcd 중지
mv /etc/kubernetes/manifests/etcd.yaml /etc/kubernetes/manifests/etcd.yaml.bak

2단계: etcd 데이터 백업
mv /var/lib/etcd /var/lib/etcd-old

3단계: etcd 스냅샷 복원 (한 노드에서만) - 기존에 etcd.yaml 파일에서 정보를 확인(1단계전 작업하기)
ETCDCTL_API=3 etcdctl snapshot restore /home/kei/backup/db.snapshot \
  --name=<etcd.yaml에 이름과 같게> \
  --initial-cluster=<etcd.yaml과 같게> \
  --initial-advertise-peer-urls=<etcd.yaml과 같게> \
  --initial-cluster-token=etcd-cluster-1 \(생략해도 됨)
  --data-dir=/var/lib/etcd
** initial-cluster는 복구 시점에 멤버 1개만 있어야 하므로 etcd-1만 명시 **
ㅣㄴ

4단계: inventory.ini 수정 (임시로 etcd 멤버를 1개로 제한)
[etcd]
master-1 ansible_host=10.0.0.1

5단계: Kubespray로 복구한 etcd를 기준으로 클러스터 복원
ansible-playbook -i inventory/dev/inventory.ini <적절한 경로/cluster.yml> -b -v --tags=etcd -K

6단계: inventory.ini에 나머지 etcd 멤버 추가
[etcd]
master-1 ansible_host=10.0.0.1
master-2 ansible_host=10.0.0.2
master-3 ansible_host=10.0.0.3

7단계: 나머지 멤버 조인 (etcd member add + Kubespray로 구성)
ansible-playbook -i inventory/dev/inventory.ini <적절한 경로/cluster.yml> -b -v --tags=etcd -K

8단계: 클러스터 복구 확인
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=... --cert=... --key=...

kubectl get nodes
kubectl get pods -A


